{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1425208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "import re\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eab74916",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DebertaRegressor(nn.Module):\n",
    "    def __init__(self, model_name=\"microsoft/deberta-v3-base\"):\n",
    "        super().__init__()\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        self.deberta = AutoModel.from_pretrained(model_name, config=self.config)\n",
    "        self.regressor = nn.Linear(self.config.hidden_size, 1)\n",
    "        self.base_model = self.deberta\n",
    "\n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output.last_hidden_state\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size())\n",
    "        return (token_embeddings * input_mask_expanded).sum(1) / input_mask_expanded.sum(1)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, **kwargs):\n",
    "        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask, **kwargs)\n",
    "        pooled_output = self.mean_pooling(outputs, attention_mask)\n",
    "        logits = self.regressor(pooled_output).squeeze(-1)\n",
    "        return torch.sigmoid(logits)\n",
    "\n",
    "class DebertaRegressorLarge(nn.Module):\n",
    "    def __init__(self, model_name=\"microsoft/deberta-v3-large\"):\n",
    "        super().__init__()\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        self.deberta = AutoModel.from_pretrained(model_name, config=self.config)\n",
    "        self.regressor = nn.Linear(self.config.hidden_size, 1)\n",
    "        self.base_model = self.deberta\n",
    "\n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output.last_hidden_state\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size())\n",
    "        return (token_embeddings * input_mask_expanded).sum(1) / input_mask_expanded.sum(1)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, **kwargs):\n",
    "        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask, **kwargs)\n",
    "        pooled_output = self.mean_pooling(outputs, attention_mask)\n",
    "        logits = self.regressor(pooled_output).squeeze(-1)\n",
    "        return torch.sigmoid(logits)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)              \n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text) \n",
    "    text = re.sub(r\"[^a-z0-9.,!?'\\s]\", ' ', text)      \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()           \n",
    "    return text\n",
    "\n",
    "def predict_feedback(model, tokenizer, text, device, max_length=512):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    text_cleaned = clean_text(text)\n",
    "\n",
    "    tokens = tokenizer(\n",
    "        text_cleaned,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    input_ids = tokens[\"input_ids\"].to(device)\n",
    "    attention_mask = tokens[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    predicted_score = pred.item()\n",
    "\n",
    "    \n",
    "    if predicted_score >= 0.7:\n",
    "        overall = \"This essay is strong overall, showing excellent organization, development of ideas, and clarity in expression.\"\n",
    "        strengths = \"Ideas are well-structured, arguments are persuasive, and language use is mostly mature and accurate.\"\n",
    "        improvements = \"Polish minor grammar issues and ensure transitions are smooth to reach perfection.\"\n",
    "\n",
    "    elif predicted_score >= 0.5:\n",
    "        overall = \"This essay is fairly effective, with generally clear ideas and good effort in structure.\"\n",
    "        strengths = \"The essay presents organized thoughts and some persuasive arguments.\"\n",
    "        improvements = \"Work on refining grammar and strengthening transitions between points.\"\n",
    "\n",
    "    elif predicted_score >= 0.3:\n",
    "        overall = \"This essay has potential but needs improvement in structure, clarity, and development.\"\n",
    "        strengths = \"There are some clear ideas, and an effort to support them is noticeable.\"\n",
    "        improvements = \"Improve sentence flow, avoid repetition, and develop arguments more fully.\"\n",
    "\n",
    "    else:\n",
    "        overall = \"This essay is underdeveloped and difficult to follow, with significant issues in structure and language.\"\n",
    "        strengths = \"The attempt to engage with the topic is visible.\"\n",
    "        improvements = \"Focus on organizing your thoughts clearly, using proper grammar, and fully developing your main ideas.\"\n",
    "\n",
    "    return {\n",
    "        \"predicted_score\": predicted_score,\n",
    "        \"overall_evaluation\": overall,\n",
    "        \"strengths\": strengths,\n",
    "        \"areas_for_improvement\": improvements\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57e68b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mohamed\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForFeatureExtraction(\n",
       "  (base_model): LoraModel(\n",
       "    (model): DebertaRegressorLarge(\n",
       "      (deberta): DebertaV2Model(\n",
       "        (embeddings): DebertaV2Embeddings(\n",
       "          (word_embeddings): Embedding(128100, 1024, padding_idx=0)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "          (dropout): StableDropout()\n",
       "        )\n",
       "        (encoder): DebertaV2Encoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-23): 24 x DebertaV2Layer(\n",
       "              (attention): DebertaV2Attention(\n",
       "                (self): DisentangledSelfAttention(\n",
       "                  (query_proj): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (key_proj): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (value_proj): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (pos_dropout): StableDropout()\n",
       "                  (dropout): StableDropout()\n",
       "                )\n",
       "                (output): DebertaV2SelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "                  (dropout): StableDropout()\n",
       "                )\n",
       "              )\n",
       "              (intermediate): DebertaV2Intermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): DebertaV2Output(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "                (dropout): StableDropout()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (rel_embeddings): Embedding(512, 1024)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (regressor): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=1024, out_features=1, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=1024, out_features=1, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (base_model): DebertaV2Model(\n",
       "        (embeddings): DebertaV2Embeddings(\n",
       "          (word_embeddings): Embedding(128100, 1024, padding_idx=0)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "          (dropout): StableDropout()\n",
       "        )\n",
       "        (encoder): DebertaV2Encoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-23): 24 x DebertaV2Layer(\n",
       "              (attention): DebertaV2Attention(\n",
       "                (self): DisentangledSelfAttention(\n",
       "                  (query_proj): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (key_proj): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (value_proj): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (pos_dropout): StableDropout()\n",
       "                  (dropout): StableDropout()\n",
       "                )\n",
       "                (output): DebertaV2SelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "                  (dropout): StableDropout()\n",
       "                )\n",
       "              )\n",
       "              (intermediate): DebertaV2Intermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): DebertaV2Output(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "                (dropout): StableDropout()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (rel_embeddings): Embedding(512, 1024)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer_base = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "tokenizer_large = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "base_model = DebertaRegressor().to(device)\n",
    "base_model_2 = DebertaRegressorLarge().to(device)\n",
    "base_model_3 = DebertaRegressorLarge().to(device)\n",
    "base_model_4 = DebertaRegressorLarge().to(device)\n",
    "model = PeftModel.from_pretrained(base_model, \"D:/nlu/lora_deberta_adapter_final\").to(device)\n",
    "model_2 = PeftModel.from_pretrained(base_model_2, \"D:/nlu/lora_deberta_adapter_final_2\").to(device)\n",
    "model_3 = PeftModel.from_pretrained(base_model_3, \"D:/nlu/lora_deberta_adapter_final_3\").to(device)\n",
    "model.eval()\n",
    "model_2.eval()\n",
    "model_3.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bfd966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deberta Base weighted\n",
      "Predicted Score: 0.6751041412353516\n",
      "Overall Evaluation: This essay is fairly effective, with generally clear ideas and good effort in structure.\n",
      "Strengths: The essay presents organized thoughts and some persuasive arguments.\n",
      "Areas for Improvement: Work on refining grammar and strengthening transitions between points.\n",
      "Deberta Large weighted\n",
      "Predicted Score: 0.7091741561889648\n",
      "Overall Evaluation: This essay is strong overall, showing excellent organization, development of ideas, and clarity in expression.\n",
      "Strengths: Ideas are well-structured, arguments are persuasive, and language use is mostly mature and accurate.\n",
      "Areas for Improvement: Polish minor grammar issues and ensure transitions are smooth to reach perfection.\n",
      "Deberta Large Balanced\n",
      "Predicted Score: 0.8171554207801819\n",
      "Overall Evaluation: This essay is strong overall, showing excellent organization, development of ideas, and clarity in expression.\n",
      "Strengths: Ideas are well-structured, arguments are persuasive, and language use is mostly mature and accurate.\n",
      "Areas for Improvement: Polish minor grammar issues and ensure transitions are smooth to reach perfection.\n",
      "Base Deberta model\n",
      "Predicted Score: 0.6967768669128418\n",
      "Overall Evaluation: This essay is fairly effective, with generally clear ideas and good effort in structure.\n",
      "Strengths: The essay presents organized thoughts and some persuasive arguments.\n",
      "Areas for Improvement: Work on refining grammar and strengthening transitions between points.\n"
     ]
    }
   ],
   "source": [
    "# Perfect essay\n",
    "essay = \"\"\"\n",
    "In today’s increasingly complex economic landscape, financial literacy is not a luxury—it is a necessity. Despite this, many high school graduates are unequipped to handle basic financial tasks such as budgeting, saving, or understanding credit. To bridge this critical gap, financial literacy should be a mandatory component of every high school curriculum. Doing so will empower students to make informed decisions, avoid debt traps, and become financially responsible adults.\n",
    "\n",
    "First, requiring financial literacy in high school equips students with essential life skills. According to a 2023 survey by the National Financial Educators Council, 64% of young adults felt that their lack of financial knowledge had cost them money. Teaching students how to create a budget, understand interest rates, and manage debt will not only benefit them personally but will also have long-term societal impacts by reducing poverty and reliance on public assistance.\n",
    "\n",
    "Second, financial literacy promotes responsible consumer behavior. In an age where credit cards, student loans, and online shopping are readily accessible, young people are more vulnerable than ever to financial missteps. A well-designed course can teach students about compound interest, credit scores, and the dangers of predatory lending. With this knowledge, they are more likely to make informed choices and avoid high-risk financial behavior.\n",
    "\n",
    "Finally, making financial education mandatory can help close the wealth gap. Studies show that students from lower-income families are less likely to receive financial education at home. By integrating it into public education, we create a more level playing field and offer every student—regardless of background—the opportunity to succeed financially.\n",
    "\n",
    "In conclusion, requiring financial literacy in high school is a powerful and practical step toward building a more financially stable and informed society. It prepares students for real-world challenges, fosters responsible habits, and promotes equity. In a world driven by money, understanding how to manage it is not just important—it is essential.\n",
    "\"\"\"\n",
    "print(\"Deberta Base weighted\")\n",
    "result = predict_feedback(model, tokenizer_base, essay, device)\n",
    "print(\"Predicted Score:\", result['predicted_score'])\n",
    "print(\"Overall Evaluation:\", result['overall_evaluation'])\n",
    "print(\"Strengths:\", result['strengths'])\n",
    "print(\"Areas for Improvement:\", result['areas_for_improvement'])\n",
    "\n",
    "print(\"Deberta Large weighted\")\n",
    "result = predict_feedback(model_2, tokenizer_large, essay, device)\n",
    "print(\"Predicted Score:\", result['predicted_score'])\n",
    "print(\"Overall Evaluation:\", result['overall_evaluation'])\n",
    "print(\"Strengths:\", result['strengths'])\n",
    "print(\"Areas for Improvement:\", result['areas_for_improvement'])\n",
    "\n",
    "print(\"Deberta Large Balanced\")\n",
    "result = predict_feedback(model_3, tokenizer_large, essay, device)\n",
    "print(\"Predicted Score:\", result['predicted_score'])\n",
    "print(\"Overall Evaluation:\", result['overall_evaluation'])\n",
    "print(\"Strengths:\", result['strengths'])\n",
    "print(\"Areas for Improvement:\", result['areas_for_improvement'])\n",
    "\n",
    "print(\"Base Deberta model\")\n",
    "result = predict_feedback(base_model_4, tokenizer_large, essay, device)\n",
    "print(\"Predicted Score:\", result['predicted_score'])\n",
    "print(\"Overall Evaluation:\", result['overall_evaluation'])\n",
    "print(\"Strengths:\", result['strengths'])\n",
    "print(\"Areas for Improvement:\", result['areas_for_improvement'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52b999a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deberta Base weighted\n",
      "Predicted Score: 0.49404025077819824\n",
      "Overall Evaluation: This essay has potential but needs improvement in structure, clarity, and development.\n",
      "Strengths: There are some clear ideas, and an effort to support them is noticeable.\n",
      "Areas for Improvement: Improve sentence flow, avoid repetition, and develop arguments more fully.\n",
      "Deberta Large weighted\n",
      "Predicted Score: 0.011292065493762493\n",
      "Overall Evaluation: This essay is underdeveloped and difficult to follow, with significant issues in structure and language.\n",
      "Strengths: The attempt to engage with the topic is visible.\n",
      "Areas for Improvement: Focus on organizing your thoughts clearly, using proper grammar, and fully developing your main ideas.\n",
      "Deberta Large Balanced\n",
      "Predicted Score: 0.007009458728134632\n",
      "Overall Evaluation: This essay is underdeveloped and difficult to follow, with significant issues in structure and language.\n",
      "Strengths: The attempt to engage with the topic is visible.\n",
      "Areas for Improvement: Focus on organizing your thoughts clearly, using proper grammar, and fully developing your main ideas.\n",
      "Base Deberta model\n",
      "Predicted Score: 0.6380103826522827\n",
      "Overall Evaluation: This essay is fairly effective, with generally clear ideas and good effort in structure.\n",
      "Strengths: The essay presents organized thoughts and some persuasive arguments.\n",
      "Areas for Improvement: Work on refining grammar and strengthening transitions between points.\n"
     ]
    }
   ],
   "source": [
    "# Bad essay\n",
    "essay = \"\"\"The sky is blue and water is wet. I have a pencil that I use to write sometimes when I think about things like clouds or maybe animals. This is writing. I like pizza. Sometimes people say stuff about important things but I don’t know what they mean. Writing is hard when the sun is shining and it’s hot. That’s all I know today.\"\"\"\n",
    "\n",
    "print(\"Deberta Base weighted\")\n",
    "result = predict_feedback(model, tokenizer_base, essay, device)\n",
    "print(\"Predicted Score:\", result['predicted_score'])\n",
    "print(\"Overall Evaluation:\", result['overall_evaluation'])\n",
    "print(\"Strengths:\", result['strengths'])\n",
    "print(\"Areas for Improvement:\", result['areas_for_improvement'])\n",
    "\n",
    "print(\"Deberta Large weighted\")\n",
    "result = predict_feedback(model_2, tokenizer_large, essay, device)\n",
    "print(\"Predicted Score:\", result['predicted_score'])\n",
    "print(\"Overall Evaluation:\", result['overall_evaluation'])\n",
    "print(\"Strengths:\", result['strengths'])\n",
    "print(\"Areas for Improvement:\", result['areas_for_improvement'])\n",
    "\n",
    "print(\"Deberta Large Balanced\")\n",
    "result = predict_feedback(model_3, tokenizer_large, essay, device)\n",
    "print(\"Predicted Score:\", result['predicted_score'])\n",
    "print(\"Overall Evaluation:\", result['overall_evaluation'])\n",
    "print(\"Strengths:\", result['strengths'])\n",
    "print(\"Areas for Improvement:\", result['areas_for_improvement'])\n",
    "\n",
    "print(\"Base Deberta model\")\n",
    "result = predict_feedback(base_model_4, tokenizer_large, essay, device)\n",
    "print(\"Predicted Score:\", result['predicted_score'])\n",
    "print(\"Overall Evaluation:\", result['overall_evaluation'])\n",
    "print(\"Strengths:\", result['strengths'])\n",
    "print(\"Areas for Improvement:\", result['areas_for_improvement'])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
